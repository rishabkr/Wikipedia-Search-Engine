{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings=['20, t:catcher in the rye, i:axl rose, b:madagascar, c:2008 albums','30, b:Taxicab 1729',\n",
    "'50, b:Marc Spector, i:Marvel Comics, c:1980 comics debuts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=strings[2].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b:Marc Spector  i:Marvel Comics  c:1980 comics debuts'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(splits[1:]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' t:catcher in the rye  i:axl rose  b:madagascar  c:2008 albums'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(splits[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as etree\n",
    "import nltk\n",
    "nltk_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "# INDEX_FILE='inverted_index.txt'\n",
    "# WIKI_XML_FILE_NAME='enwiki-latest-pages-articles2.xml'\n",
    "\n",
    "# sample_xml='sample.xml'\n",
    "# title_file='title_index.txt'\n",
    "class Wiki_Indexer:\n",
    "    def __init__(self):\n",
    "        self.title_index=defaultdict(list)\n",
    "        self.text_index=defaultdict(list)\n",
    "        self.category_index=defaultdict(list)\n",
    "        #title_tag_file=open(title_file,'w',encoding='utf-8')\n",
    "        \n",
    "        self.title_positions=[]\n",
    "        self.stopwords=defaultdict()\n",
    "        self.infobox_index=defaultdict(list)\n",
    "        self.reference_index=defaultdict(list)\n",
    "        self.stemmed_words={}\n",
    "        self.css_pattern=re.compile(r'{\\|(.*?)\\|}',re.DOTALL)\n",
    "        self.file_pattern = re.compile(r'\\[\\[file:(.*?)\\]\\]',re.DOTALL)\n",
    "        self.url_pattern=re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',re.DOTALL)\n",
    "        \n",
    "        self.STOPWORDS_FILE='stopwords2.txt'\n",
    "\n",
    "        self.title_index_file=open('title_names.txt','w',encoding='utf-8')\n",
    "\n",
    "        self.total_tokens_in_dump=0\n",
    "        self.total_tokens_in_index=0\n",
    "        \n",
    "        self.page_count=0\n",
    "        \n",
    "        self.dump_size=0\n",
    "\n",
    "   \n",
    "        self.num_file=0\n",
    "\n",
    "        self.num_files=12\n",
    "    \n",
    "    def load_stopwords(self):\n",
    "            stopwords_file=open(self.STOPWORDS_FILE,'r')\n",
    "            stpwords=[line.rstrip() for line in stopwords_file]\n",
    "            for word in stpwords:\n",
    "                self.stopwords[word]=True\n",
    "            stopwords_file.close()\n",
    "    \n",
    "    def add_to_stem_dict(self,word):\n",
    "       \n",
    "        self.stemmed_words[word]=nltk_stemmer.stem(word)\n",
    "        return self.stemmed_words[word]\n",
    "        \n",
    "    def parse_category_box(self,text,category_count):\n",
    "            categories=re.findall(\"\\[\\[Category:(.*?)\\]\\]\",text)\n",
    "      \n",
    "            for category in categories:\n",
    "                text_pattern=re.compile(r'[a-zA-Z]+|[0-9]{1,4}') \n",
    "                category_words=re.findall(text_pattern,category)\n",
    "                category_words=list(map(lambda x:x.lower(),category_words))\n",
    "                \n",
    "                category_words=[x for x in category_words if (x!=\"\" and (not x in self.stopwords and len(x)) and len(x)>2)]\n",
    "                #self.total_tokens_in_dump+=len(category_words)\n",
    "                category_words=[self.stemmed_words[word] if word in self.stemmed_words else self.add_to_stem_dict(word) for word in category_words]\n",
    "                # print(category_words)\n",
    "                for word in category_words:\n",
    "                     category_count[word]+=1\n",
    "            \n",
    "            return category_count\n",
    "    \n",
    "\n",
    "        \n",
    "    def parse_infobox_text(self,text,infobox_page_count):\n",
    "     \n",
    "                infobox_text=[]\n",
    "                flag=False\n",
    "                for line in text.split('\\n'):\n",
    "                    if line.startswith(\"{{Infobox\"):\n",
    "                        flag=True\n",
    "                        infobox_text.append(line)\n",
    "                    elif flag and line!=\"}}\":\n",
    "                        infobox_text.append(line)\n",
    "                    elif flag:\n",
    "                        break\n",
    "                \n",
    "                for info_text in infobox_text:\n",
    "                    text_pattern=re.compile(r'[a-zA-Z]+|[0-9]{1,4}') \n",
    "                    info_words=re.findall(text_pattern,info_text)\n",
    "                    \n",
    "                    info_words=list(map(lambda x:x.lower(),info_words))\n",
    "                    \n",
    "                    info_words=[x for x in info_words if (x!=\"\" and (not x in self.stopwords)and len(x)>2)]\n",
    "                    #self.total_tokens_in_dump+=len(info_words)\n",
    "\n",
    "                    info_words=[self.stemmed_words[word] if word in self.stemmed_words else self.add_to_stem_dict(word) for word in info_words]\n",
    "                    for word in info_words:\n",
    "                        infobox_page_count[word]+=1\n",
    "                return infobox_page_count\n",
    "    \n",
    "                \n",
    "    def parse_main_text(self,text,main_text_count,category_count,infobox_page_count,reference_count):\n",
    "            #text=css_pattern.sub('',text)\n",
    "            body=text.lower()\n",
    "           \n",
    "            text_pattern=re.compile(r'[a-zA-Z]+|[0-9]{1,4}') \n",
    "            body=re.findall(text_pattern,body)\n",
    "            body_words=[x for x in body if (x!=\"\" and (not x in self.stopwords)and len(x)>2)]\n",
    "            \n",
    "            self.total_tokens_in_dump+=len(body_words)\n",
    "\n",
    "            body_words=[self.stemmed_words[word] if word in self.stemmed_words else self.add_to_stem_dict(word) for word in body_words]\n",
    "            for word in body_words:\n",
    "                main_text_count[word]+=1\n",
    "            \n",
    "            for word in main_text_count.keys():\n",
    "                 \n",
    "                if(category_count.get(word) is not None):\n",
    "                    main_text_count[word]-=category_count.get(word)\n",
    "\n",
    "                if(infobox_page_count.get(word) is not None):\n",
    "                    main_text_count[word]-=infobox_page_count.get(word)\n",
    "\n",
    "\n",
    "                if(reference_count.get(word)is not None):\n",
    "                    main_text_count[word]-=reference_count.get(word)\n",
    "\n",
    "\n",
    "            return main_text_count\n",
    "                \n",
    "    def parse_title(self,title,title_count):\n",
    "\n",
    "            ttle=title\n",
    "            ttle+='\\n'\n",
    "            \n",
    "            body=title.lower()\n",
    "            text_pattern=re.compile(r'[a-zA-Z]+|[0-9]{1,4}') \n",
    "            body=re.findall(text_pattern,body)\n",
    "            body_words=[x for x in body if (x!=\"\" and len(x)>2 and (not x in self.stopwords))]\n",
    "            \n",
    "            self.total_tokens_in_dump+=len(body_words)\n",
    "            body_words=[self.stemmed_words[word] if word in self.stemmed_words else self.add_to_stem_dict(word) for word in body_words]\n",
    "            \n",
    "            for word in body_words:\n",
    "                title_count[word]+=1\n",
    "            return title_count\n",
    "\n",
    "    def parse_for_references(self,text,reference_count):\n",
    "  \n",
    "\n",
    "        text_pattern=re.compile(\"[a-zA-Z\\d]+\")\n",
    "        st1=re.sub('&lt;ref&gt;','<ref>',text)\n",
    "        st2=re.sub('&lt;/ref&gt;','</ref>',st1)\n",
    "        lists=re.findall('\\<ref\\>([^\\<]+)\\<\\/ref\\>',st2)\n",
    "        \n",
    "        words=[re.findall(text_pattern,element) for element in lists]\n",
    "        nonempty_words=[[word for word in list1 if(word!=\"\" and len(word)>2 and (not word in self.stopwords))] for list1 in words]\n",
    "        words=[word.lower() for lists in nonempty_words for word in lists]\n",
    "        #self.total_tokens_in_dump+=len(words)\n",
    "\n",
    "        words=[self.stemmed_words[word] if word in self.stemmed_words else self.add_to_stem_dict(word) for word in words]\n",
    "        \n",
    "        for word in words:\n",
    "            reference_count[word]+=1\n",
    "        return reference_count\n",
    "    \n",
    "    def write_title_index(self,id_,count,title_count):\n",
    "         \n",
    "            for word in title_count:\n",
    "                word_count=str(title_count[word])\n",
    "                page_number=str(count)\n",
    "                word_and_page_count=id_+':'+word_count\n",
    "                self.title_index[word].append(word_and_page_count)\n",
    "        \n",
    "    def write_text_index(self,id_,count,main_text_count):\n",
    "           for word in main_text_count:\n",
    "                if main_text_count[word]>0:\n",
    "                    word_count=str(main_text_count[word])\n",
    "                    page_number=str(count)\n",
    "                    word_and_page_count=id_+':'+word_count\n",
    "                    self.text_index[word].append(word_and_page_count)\n",
    "        \n",
    "        \n",
    "    def write_category_index(self,id_,count,category_count):\n",
    "            for word in category_count:\n",
    "                if category_count[word]>0:\n",
    "                    word_count=str(category_count[word])\n",
    "                    page_number=str(count)\n",
    "                    word_and_page_count=id_+':'+word_count\n",
    "                    self.category_index[word].append(word_and_page_count)\n",
    "        \n",
    "\n",
    "    def write_reference_index(self,id_,count,reference_count):\n",
    "          \n",
    "            for word in reference_count:\n",
    "            \tif reference_count[word]>0:\n",
    "\t                word_count=str(reference_count[word])\n",
    "\t                page_number=str(count)\n",
    "\t                word_and_page_count=id_+':'+word_count\n",
    "\t                self.reference_index[word].append(word_and_page_count)\n",
    "       \n",
    "\n",
    "                \n",
    "    def write_infobox_index(self,id_,count,infobox_page_count):\n",
    "            for word in infobox_page_count:\n",
    "                if infobox_page_count[word]!=0:\n",
    "                    word_count=str(infobox_page_count[word])\n",
    "                    page_number=str(count)\n",
    "                    word_page_count=id_+':'+word_count\n",
    "\n",
    "                    self.infobox_index[word].append(word_page_count)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "            param=sys.argv\n",
    "            self.collectionFile='xml_dump.xml'#param[1]\n",
    "            self.indexFile='index_file.txt'#param[2]\n",
    "            self.stat_file='index_stat.txt'#param[3]\n",
    "            \n",
    "    def parse_xml_file(self):\n",
    "            from time import time\n",
    "\n",
    "            \n",
    "            #self.get_params()\n",
    "            self.load_stopwords()\n",
    "            count=0\n",
    "            revsn_tag=False\n",
    "            #WIKI_XML_FILE_NAME\n",
    "            \n",
    "            self.num_file=1\n",
    "            dump_names=os.listdir(sys.argv[1])\n",
    "            file_name=sys.argv[1]+'/'\n",
    "            print(dump_names)\n",
    "            for i in range(len(dump_names)):\n",
    "                #self.collectionFile=file_name+'_'+str(i)+'.xml'\n",
    "                self.collectionFile=dump_names[i]\n",
    "          \n",
    "                print(f'Processing dump number: {i}')\n",
    "                start=time()\n",
    "                with open(file_name+self.collectionFile,encoding='utf-8') as xmlfile:\n",
    "                    context=etree.iterparse(xmlfile,events=('start','end'))\n",
    "                    context=iter(context)\n",
    "                    \n",
    "                    for event, element in tqdm(context):\n",
    "                        tag_name=re.sub(r'{.*}','',element.tag)\n",
    "                        if event=='start':\n",
    "                            if tag_name=='page':\n",
    "                                category_count=defaultdict(int)\n",
    "                                title_count=defaultdict(int)\n",
    "                                main_text_count=defaultdict(int)\n",
    "                                infobox_page_count=defaultdict(int)\n",
    "                                reference_count=defaultdict(int)\n",
    "                                revsn_tag=False\n",
    "                                self.page_count+=1\n",
    "                            elif tag_name=='revision':\n",
    "                                revsn_tag=True\n",
    "\n",
    "                        else:\n",
    "                            if tag_name=='title' :\n",
    "                                title_text=element.text\n",
    "                                title_count=self.parse_title(title_text,title_count)\n",
    "\n",
    "                            elif tag_name=='id' and not revsn_tag:\n",
    "                                ids=int(element.text)\n",
    "\n",
    "                            elif tag_name=='text':\n",
    "                                try:\n",
    "                                    current_text=element.text\n",
    "                                    current_text=self.css_pattern.sub('',current_text)\n",
    "                                    current_text=self.file_pattern.sub('',current_text)\n",
    "                                    current_text=self.url_pattern.sub('',current_text)\n",
    "\n",
    "                                    category_count=self.parse_category_box(current_text,category_count)\n",
    "                            \n",
    "                                    infobox_page_count=self.parse_infobox_text(current_text,infobox_page_count)\n",
    "                                    reference_count=self.parse_for_references(current_text,reference_count)\n",
    "                                    main_text_count=self.parse_main_text(current_text,main_text_count,category_count,infobox_page_count,reference_count)\n",
    "                            \n",
    "                                except:\n",
    "                                    pass\n",
    "\n",
    "                            elif tag_name=='page':\n",
    "                                self.write_title_index(str(ids),count,title_count)\n",
    "                                self.write_category_index(str(ids),count,category_count)\n",
    "                                self.write_text_index(str(ids),count,main_text_count)\n",
    "                                self.write_infobox_index(str(ids),count,infobox_page_count)\n",
    "                                self.write_reference_index(str(ids),count,reference_count)\n",
    "                                self.title_index_file.write(str(ids)+'#'+title_text+'\\n')\n",
    "                         \n",
    "\n",
    "                                if(self.page_count%30000==0):\n",
    "                                    self.write_index_files(self.num_file)\n",
    "                                    self.num_file+=1\n",
    "                                    self.text_index.clear()\n",
    "                                    self.title_index.clear()\n",
    "                                    self.infobox_index.clear()\n",
    "                                    self.category_index.clear()\n",
    "                                    self.reference_index.clear()\n",
    "                                \n",
    "                                if(self.page_count%50000==0):\n",
    "                                    self.stemmed_words.clear()\n",
    "\n",
    "                            element.clear()\n",
    "\n",
    "                   \n",
    "\n",
    "                    end=time()\n",
    "                    print(f'Parsing successful!! in {end-start} seconds ')\n",
    "                    \n",
    "                    self.write_index_files(self.num_file)\n",
    "                    self.num_file+=1\n",
    "\n",
    "                    self.text_index.clear()\n",
    "                    self.title_index.clear()\n",
    "                    self.infobox_index.clear()\n",
    "                    self.category_index.clear()\n",
    "                    self.reference_index.clear()\n",
    "\n",
    "                    xmlfile.close()\n",
    "            self.title_index_file.close()\n",
    "                    \n",
    "                    #title_tag_file.close()\n",
    "\n",
    "\n",
    "    def write_index_files(self,idx):\n",
    "\n",
    "            category_set=sorted(set(self.category_index.keys()))\n",
    "            title_set=sorted(set(self.title_index.keys()))\n",
    "            text_set=sorted(set(self.text_index.keys()))\n",
    "            reference_set=sorted(set(self.reference_index.keys()))\n",
    "            infobox_set=sorted(set(self.infobox_index.keys()))\n",
    "\n",
    "\n",
    "            for word in self.stopwords.keys():\n",
    "                    try:\n",
    "                        category_set.remove(word)\n",
    "                        title_set.remove(word)\n",
    "                        text_set.remove(word)\n",
    "                        reference_set.remove(word)\n",
    "                        infobox_set.remove(word)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            body_directory='indexes/body/'\n",
    "            category_directory='indexes/category/'\n",
    "            references_directory='indexes/references/'\n",
    "            infobox_directory='indexes/infobox/'\n",
    "            title_directory='indexes/titles/'\n",
    "\n",
    "            category_index_file=category_directory+'category_index_'+str(idx)+'.txt'\n",
    "            infobox_index_file=infobox_directory+'infobox_index_'+str(idx)+'.txt'\n",
    "            reference_index_file=references_directory+'reference_index_'+str(idx)+'.txt'\n",
    "            body_index_file=body_directory+'body_index_'+str(idx)+'.txt'\n",
    "            title_index_file=title_directory+'title_index_'+str(idx)+'.txt'\n",
    "\n",
    "            cif=open(category_index_file,'w',encoding='utf-8')\n",
    "            iif=open(infobox_index_file,'w',encoding='utf-8')\n",
    "            rif=open(reference_index_file,'w',encoding='utf-8')\n",
    "            bif=open(body_index_file,'w',encoding='utf-8')\n",
    "            tif=open(title_index_file,'w',encoding='utf-8')\n",
    "\n",
    "            self.total_tokens_in_index=self.total_tokens_in_index+len(category_set)+len(title_set)+len(text_set)+len(reference_set)+len(infobox_set)\n",
    "              \n",
    "            for i in category_set:\n",
    "                            cat_posting=self.category_index[i]\n",
    "                            cat_posting=','.join(cat_posting)\n",
    "                            if cat_posting:\n",
    "                                cif.write(f'{i}|{cat_posting}\\n')\n",
    "\n",
    "            for i in infobox_set:\n",
    "                            info_posting=self.infobox_index[i]\n",
    "                            info_posting=','.join(info_posting)\n",
    "                            if info_posting:\n",
    "                                iif.write(f'{i}|{info_posting}\\n')\n",
    "            \n",
    "            for i in reference_set:\n",
    "                            reference_posting=self.reference_index[i]\n",
    "                            reference_posting=','.join(reference_posting)\n",
    "                            if reference_posting:\n",
    "                                rif.write(f'{i}|{reference_posting}\\n')\n",
    "            \n",
    "            for i in  text_set:\n",
    "                            text_posting=self.text_index[i]\n",
    "                            text_posting=','.join(text_posting)\n",
    "                            if text_posting:\n",
    "                                bif.write(f'{i}|{text_posting}\\n')                    \n",
    "\n",
    "            for i in title_set:\n",
    "                            title_posting=self.title_index[i]\n",
    "                            title_posting=','.join(title_posting)\n",
    "                            if title_posting:\n",
    "                                tif.write(f'{i}|{title_posting}\\n')\n",
    "\n",
    "\n",
    "            cif.close()\n",
    "            bif.close()\n",
    "            tif.close()\n",
    "            iif.close()\n",
    "            rif.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # stat_file=open(f'index_stat_{idx}.txt','w')\n",
    "            # stat_file.write(str(self.total_tokens_in_dump)+'\\n')\n",
    "            # stat_file.write(str(self.total_tokens_in_index)+'\\n')\n",
    "            # stat_file.close()\n",
    "\n",
    "\n",
    "\n",
    "def get_size(start_path = 'indexes'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    indexer=Wiki_Indexer()\n",
    "\n",
    "    os.makedirs('indexes/body')\n",
    "    os.makedirs('indexes/titles')\n",
    "    os.makedirs('indexes/references')\n",
    "    os.makedirs('indexes/category')\n",
    "    os.makedirs('indexes/infobox')\n",
    "    \n",
    "\n",
    "    start=time()\n",
    "    indexer.parse_xml_file()\n",
    "    end=time()\n",
    "    \n",
    "\n",
    "    index_size=get_size()/2**30\n",
    "\n",
    "   \n",
    "    stat_file=open('stats.txt','w',encoding='utf-8')\n",
    "    stat_file.write(f'{index_size} GB\\n')\n",
    "    stat_file.write(f'{indexer.num_file-1}\\n')\n",
    "    stat_file.write(f'{indexer.total_tokens_in_index}\\n')\n",
    "   \n",
    "    stat_file.close()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    print(f'Total time taken is {end-start} Seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__=='__main__':\n",
    "#     indexer=Wiki_Indexer()\n",
    "\n",
    "#     os.makedirs('indexes/body')\n",
    "#     os.makedirs('indexes/titles')\n",
    "#     os.makedirs('indexes/references')\n",
    "#     os.makedirs('indexes/category')\n",
    "#     os.makedirs('indexes/infobox')\n",
    "    \n",
    "\n",
    "#     start=time()\n",
    "#     indexer.parse_xml_file()\n",
    "#     end=time()\n",
    "    \n",
    "\n",
    "#     index_size=get_size()/2**30\n",
    "\n",
    "   \n",
    "#     stat_file=open('stats.txt','w',encoding='utf-8')\n",
    "#     stat_file.write(f'{index_size} GB\\n')\n",
    "#     stat_file.write(f'{indexer.num_file-1}\\n')\n",
    "#     stat_file.write(f'{indexer.total_tokens_in_index}\\n')\n",
    "   \n",
    "#     stat_file.close()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     print(f'Total time taken is {end-start} Seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from heapq import *\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class IndexMerger:\n",
    "    def __init__(self,top_k,doc_count):\n",
    "        \n",
    "        self.body_directory='indexes/body/'\n",
    "        self.category_directory='indexes/category/'\n",
    "        self.references_directory='indexes/references/'\n",
    "        self.infobox_directory='indexes/infobox/'\n",
    "        self.title_directory='indexes/titles/'\n",
    "\n",
    "\n",
    "        self.body_files=os.listdir(self.body_directory)\n",
    "        self.body_index='final_text_index.txt'\n",
    "        \n",
    "\n",
    "        self.title_files=os.listdir(self.title_directory)\n",
    "        self.title_index='final_title_index.txt'\n",
    "\n",
    "        self.category_files=os.listdir(self.category_directory)\n",
    "        self.category_index='final_category_index.txt'\n",
    "\n",
    "        self.reference_files=os.listdir(self.references_directory)\n",
    "        self.reference_index='final_reference_index.txt'\n",
    "\n",
    "\n",
    "        self.infobox_files=os.listdir(self.infobox_directory)\n",
    "        self.infobox_index='final_infobox_index.txt'\n",
    "\n",
    "        self.top_k=top_k\n",
    "        self.num_pages=doc_count\n",
    "\n",
    "\n",
    "    def get_tf_idf(self,word,postings,index_writer,k):\n",
    "         \n",
    "        word_idf=defaultdict(float)\n",
    "\n",
    "        docs=postings.split(',')\n",
    "       \n",
    "        idf=math.log10(self.num_pages/len(docs))\n",
    "        for doc in docs:\n",
    "                doc_num,count=doc.split(':')\n",
    "                tf=1+math.log10(int(count))\n",
    "                word_idf[str(doc_num)]=round(idf*tf,2)\n",
    "\n",
    "        \n",
    "        word_idf=sorted(word_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
    " \n",
    "        final_doc=[str(item[0])+':'+str(item[1]) for item in word_idf]\n",
    "\n",
    "\n",
    "        final_doc=final_doc[:k+1]\n",
    "        \n",
    "        tf_idf_index=','.join(final_doc)+'\\n'\n",
    "        \n",
    "        tf_idf_index=word+'|'+tf_idf_index\n",
    "        \n",
    "        index_writer.write(tf_idf_index)\n",
    "\n",
    " \n",
    "    def merge_index_files(self,heap,file_pointers,index_type,index_files,k):\n",
    "        \n",
    "        final_index=open(index_type,'w',encoding='utf-8')\n",
    "        num_files=len(index_files)\n",
    "       \n",
    "        line_number=0\n",
    "        file_count=0\n",
    "        heapify(heap)\n",
    "\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            while(file_count<=num_files):\n",
    "\n",
    "                posting_list,file_number=heappop(heap)\n",
    "                word,postings=posting_list.split('|')\n",
    "            \n",
    "                fp=file_pointers[int(file_number)]\n",
    "                next_line=fp.readline().rstrip()\n",
    "                \n",
    "            \n",
    "                if next_line:\n",
    "                    heappush(heap,(next_line,file_number))\n",
    "                \n",
    "                else:\n",
    "                    fp.close()\n",
    "                    file_count=file_count+1\n",
    "\n",
    "                \n",
    "                if file_count==num_files:\n",
    "                    break\n",
    "\n",
    "            \n",
    "                while(1):\n",
    "                    posts,file_number=heappop(heap)\n",
    "                \n",
    "                    curr_word,curr_postings=posts.split('|')\n",
    "\n",
    "                    if curr_word==word:\n",
    "                        postings=postings+\",\"+curr_postings\n",
    "                        fp=file_pointers[int(file_number)]\n",
    "                        next_line=fp.readline().rstrip()\n",
    "                        \n",
    "                        if next_line:\n",
    "                            heappush(heap,(next_line,file_number))\n",
    "\n",
    "                        else:\n",
    "                            fp.close()\n",
    "                            file_count=file_count+1\n",
    "                    else:\n",
    "                        heappush(heap,(posts,file_number))\n",
    "                        break;\n",
    "\n",
    "                self.get_tf_idf(word,postings,final_index,k)\n",
    "        except:\n",
    "            pass\n",
    "        final_index.close()\n",
    "\n",
    "\n",
    "    \n",
    "    def create_scored_index(self,index_file,scored_index_file_name,top_k):\n",
    "        big_index=open(index_file,'r',encoding='utf-8')\n",
    "        large_dict=defaultdict(str)\n",
    "\n",
    "        for lines in big_index:\n",
    "            word,postings=lines.split('|')\n",
    "            if(large_dict.get(word) is not None):\n",
    "                    postings=','+postings\n",
    "            large_dict[word]+=postings.rstrip()\n",
    "        \n",
    "        big_index.close()\n",
    "\n",
    "        new_dict=defaultdict(list)\n",
    "        \n",
    "        for word in tqdm(large_dict.keys()):\n",
    "            posts=large_dict[word].split(',')\n",
    "    \n",
    "            postings=[(str(docs.split(':')[0]),float(docs.split(':')[1])) for docs in posts]\n",
    "            top=sorted(postings,key=operator.itemgetter(1),reverse=True)\n",
    "            \n",
    "            # if(len(top)>top_k):\n",
    "            #     new_dict[word]=top[:top_k+1]\n",
    "            # else:\n",
    "            #     new_dict[word]=top\n",
    "\n",
    "            new_dict[word]=top[:top_k+1]\n",
    "\n",
    "        scored_index=open(scored_index_file_name,'w',encoding='utf-8')\n",
    "\n",
    "        for word in tqdm(sorted(new_dict.keys())):\n",
    "            curr=\"\"\n",
    "            for items in new_dict[word]:\n",
    "                curr+=items[0]+':'+str(items[1])+','\n",
    "            \n",
    "            scored_index.write(word+'|'+curr.rstrip(',')+'\\n')\n",
    "\n",
    "        scored_index.close()\n",
    "\n",
    "\n",
    "    def merge_body_files(self):\n",
    "        self.body_file_pointers={}\n",
    "        \n",
    "        i=1\n",
    "        heap=[]\n",
    "\n",
    "        for fname in self.body_files:\n",
    "            fp=open(self.body_directory+fname,'r',encoding='utf-8')\n",
    "            self.body_file_pointers[i]=fp\n",
    "            first_line=fp.readline().rstrip()\n",
    "            heap.append((first_line,i))\n",
    "            i+=1\n",
    "\n",
    "        start=time()\n",
    "        self.merge_index_files(heap,self.body_file_pointers,self.body_index,self.body_files,self.top_k)\n",
    "        end=time()\n",
    "\n",
    "        print(f'Time taken to construct merged body files {end-start}')\n",
    "        \n",
    "\n",
    "        start2=time()\n",
    "       \n",
    "        self.create_scored_index(self.body_index,\"scored_body_index.txt\",self.top_k)\n",
    "\n",
    "        end2=time()\n",
    "        print(f'time taken to create scored body index {end2-start2}')\n",
    "        heap=[]\n",
    "        \n",
    "        self.clean_up(\"body\")\n",
    "\n",
    "    \n",
    "    def merge_category_files(self):\n",
    "        self.category_file_pointers={}\n",
    "        \n",
    "        i=1\n",
    "        heap=[]\n",
    "\n",
    "        for fname in self.category_files:\n",
    "            fp=open(self.category_directory+fname,'r',encoding='utf-8')\n",
    "            self.category_file_pointers[i]=fp\n",
    "            first_line=fp.readline().rstrip()\n",
    "            heap.append((first_line,i))\n",
    "            i+=1\n",
    "\n",
    "        start=time()\n",
    "        self.merge_index_files(heap,self.category_file_pointers,self.category_index,self.category_files,1000000)\n",
    "\n",
    "        end=time()\n",
    "\n",
    "        print(f'Time taken to construct merged category_files files {end-start}')\n",
    "        \n",
    "\n",
    "        start2=time()\n",
    "       \n",
    "        self.create_scored_index(self.category_index,\"scored_category_index.txt\",1000000)\n",
    "\n",
    "        end2=time()\n",
    "        print(f'time taken to create scored category index {end2-start2}')\n",
    "        heap=[]\n",
    "        \n",
    "        self.clean_up(\"category\")\n",
    "    \n",
    "    \n",
    "    def merge_infobox_files(self):\n",
    "        self.infobox_file_pointers={}\n",
    "        \n",
    "        i=1\n",
    "        heap=[]\n",
    "\n",
    "        for fname in self.infobox_files:\n",
    "            fp=open(self.infobox_directory+fname,'r',encoding='utf-8')\n",
    "            self.infobox_file_pointers[i]=fp\n",
    "            first_line=fp.readline().rstrip()\n",
    "            heap.append((first_line,i))\n",
    "            i+=1\n",
    "\n",
    "        start=time()\n",
    "        self.merge_index_files(heap,self.infobox_file_pointers,self.infobox_index,self.infobox_files,1000000)\n",
    "\n",
    "        end=time()\n",
    "\n",
    "        print(f'Time taken to construct merged infobox files {end-start}')\n",
    "        \n",
    "\n",
    "        start2=time()\n",
    "       \n",
    "        self.create_scored_index(self.infobox_index,\"scored_infobox_index.txt\",1000000)\n",
    "\n",
    "        end2=time()\n",
    "        print(f'time taken to create scored infobox index {end2-start2}')\n",
    "        heap=[]\n",
    "        \n",
    "        self.clean_up(\"infobox\")\n",
    "\n",
    "    \n",
    "    def merge_title_files(self):\n",
    "        self.title_file_pointers={}\n",
    "        \n",
    "        i=1\n",
    "        heap=[]\n",
    "\n",
    "        for fname in self.title_files:\n",
    "            fp=open(self.title_directory+fname,'r',encoding='utf-8')\n",
    "            self.title_file_pointers[i]=fp\n",
    "            first_line=fp.readline().rstrip()\n",
    "            heap.append((first_line,i))\n",
    "            i+=1\n",
    "\n",
    "        start=time()\n",
    "        self.merge_index_files(heap,self.title_file_pointers,self.title_index,self.title_files,1000000)\n",
    "\n",
    "        end=time()\n",
    "\n",
    "        print(f'Time taken to construct merged title files {end-start}')\n",
    "        \n",
    "\n",
    "        start2=time()\n",
    "       \n",
    "        self.create_scored_index(self.title_index,\"scored_title_index.txt\",1000000)\n",
    "\n",
    "        end2=time()\n",
    "        print(f'time taken to create scored title index {end2-start2}')\n",
    "        heap=[]\n",
    "        \n",
    "        self.clean_up(\"title\")\n",
    "\n",
    "\n",
    "\n",
    "    def merge_reference_files(self):\n",
    "        self.reference_file_pointers={}\n",
    "        \n",
    "        i=1\n",
    "        heap=[]\n",
    "\n",
    "        for fname in self.reference_files:\n",
    "            fp=open(self.references_directory+fname,'r',encoding='utf-8')\n",
    "            self.reference_file_pointers[i]=fp\n",
    "            first_line=fp.readline().rstrip()\n",
    "            heap.append((first_line,i))\n",
    "            i+=1\n",
    "\n",
    "        start=time()\n",
    "        self.merge_index_files(heap,self.reference_file_pointers,self.reference_index,self.reference_files,2500)\n",
    "\n",
    "        end=time()\n",
    "\n",
    "        print(f'Time taken to construct merged reference files {end-start}')\n",
    "        \n",
    "\n",
    "        start2=time()\n",
    "       \n",
    "        self.create_scored_index(self.reference_index,\"scored_reference_index.txt\",2500)\n",
    "\n",
    "        end2=time()\n",
    "        print(f'time taken to create scored reference index {end2-start2}')\n",
    "        heap=[]\n",
    "        self.clean_up(\"reference\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def clean_up(self,clean_up_type):\n",
    "        if clean_up_type==\"body\":\n",
    "\n",
    "            os.remove(self.body_index)\n",
    "\n",
    "            for i in self.body_file_pointers:\n",
    "                self.body_file_pointers[i].close()\n",
    "\n",
    "            #for fname in self.body_files:\n",
    "                #os.remove(self.body_directory+fname)\n",
    "\n",
    "        if clean_up_type==\"reference\":\n",
    "\n",
    "            os.remove(self.reference_index)\n",
    "\n",
    "            for i in self.reference_file_pointers:\n",
    "                self.reference_file_pointers[i].close()\n",
    "\n",
    "            #for fname in self.reference_files:\n",
    "                #os.remove(self.references_directory+fname)\n",
    "\n",
    "        \n",
    "        if clean_up_type==\"category\":\n",
    "\n",
    "            os.remove(self.category_index)\n",
    "\n",
    "            for i in self.category_file_pointers:\n",
    "                self.category_file_pointers[i].close()\n",
    "\n",
    "            #for fname in self.category_files:\n",
    "                #os.remove(self.category_directory+fname)\n",
    "\n",
    "\n",
    "        if clean_up_type==\"infobox\":\n",
    "\n",
    "            os.remove(self.infobox_index)\n",
    "\n",
    "            for i in self.infobox_file_pointers:\n",
    "                self.infobox_file_pointers[i].close()\n",
    "\n",
    "            #for fname in self.infobox_files:\n",
    "                #os.remove(self.infobox_directory+fname)\n",
    "\n",
    "\n",
    "        if clean_up_type==\"title\":\n",
    "\n",
    "            os.remove(self.title_index)\n",
    "\n",
    "            for i in self.title_file_pointers:\n",
    "                self.title_file_pointers[i].close()\n",
    "\n",
    "            #for fname in self.title_files:\n",
    "                #os.remove(self.title_directory+fname)\n",
    "\n",
    "\n",
    "\n",
    "    def merge_files(self):\n",
    "        self.merge_title_files()\n",
    "        self.merge_infobox_files()\n",
    "        self.merge_category_files()\n",
    "        self.merge_body_files()\n",
    "        self.merge_reference_files()\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    start=time()\n",
    "    doc_count=0\n",
    "    tf=open('title_names.txt','r',encoding='utf-8')\n",
    "    for line in tf:\n",
    "        doc_count+=1\n",
    "    tf.close()\n",
    "    print(doc_count)\n",
    "    \n",
    "    merger=IndexMerger(520,doc_count)\n",
    "    \n",
    "    merger.merge_files()\n",
    "    \n",
    "    end=time()\n",
    "\n",
    "    print(f'Total time taken to merge is {end-start} seconds!! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__=='__main__':\n",
    "    \n",
    "#     doc_count=0\n",
    "#     tf=open('title_names.txt','r',encoding='utf-8')\n",
    "#     for line in tf:\n",
    "#     \tdoc_count+=1\n",
    "#     tf.close()\n",
    "#     print(doc_count)\n",
    "    \n",
    "#     merger=IndexMerger(120,doc_count)\n",
    "\n",
    "#     merger.merge_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def write_index_files(self,idx):\n",
    "\n",
    "#             final_set=set(sorted(set.union(set(self.category_index.keys()),set(self.title_index.keys()),set(self.reference_index.keys()),set(self.text_index.keys()),set(self.infobox_index.keys()))))\n",
    "            \n",
    "#             for word in self.stopwords.keys():\n",
    "#                     try:\n",
    "#                         final_set.remove(word)\n",
    "#                     except:\n",
    "#                         pass\n",
    "      \n",
    "#             # big_index='big_index.txt'\n",
    "#             # big_index_file=open(self.indexFile,'w')\n",
    "#             final_set=sorted(final_set)\n",
    "#             self.total_tokens_in_index+=len(final_set)\n",
    "#             index_file='index_num'+str(idx)+'.txt'\n",
    "#             with open(index_file,'w',encoding='utf-8') as big_index_file:\n",
    "#                     for i in final_set:\n",
    "#                         cat_posting=self.category_index[i]\n",
    "#                         cat_posting=','.join(cat_posting)\n",
    "#                         if not cat_posting:\n",
    "#                             cat_posting=\"\"\n",
    "\n",
    "#                         info_posting=self.infobox_index[i]\n",
    "#                         info_posting=','.join(info_posting)\n",
    "#                         if not info_posting:\n",
    "#                             info_posting=\"\"\n",
    "                        \n",
    "#                         text_posting=self.text_index[i]\n",
    "#                         text_posting=','.join(text_posting)\n",
    "#                         if not text_posting:\n",
    "#                             text_posting=''\n",
    "\n",
    "#                         title_posting=self.title_index[i]\n",
    "#                         title_posting=','.join(title_posting)\n",
    "#                         if not title_posting:\n",
    "#                             title_posting=''\n",
    "                    \n",
    "#                         reference_posting=self.reference_index[i]\n",
    "#                         reference_posting=','.join(reference_posting)\n",
    "#                         if not reference_posting:\n",
    "#                             reference_posting=''\n",
    "                \n",
    "#                         to_write=f'{i}|c:{cat_posting};i:{info_posting};b:{text_posting};t:{title_posting};r:{reference_posting}\\n'\n",
    "                        \n",
    "#                         #tp.write(f'{i}:{str(big_index_file.tell())}\\n')\n",
    "                        \n",
    "#                         big_index_file.write(to_write)                    \n",
    "#             print('Index File written')\n",
    "#             big_index_file.close()\n",
    "           \n",
    "#             stat_file=open(f'index_stat_{idx}.txt','w')\n",
    "#             stat_file.write(str(self.total_tokens_in_dump)+'\\n')\n",
    "#             stat_file.write(str(self.total_tokens_in_index)+'\\n')\n",
    "#             stat_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as etree\n",
    "nltk_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import operator\n",
    "from time import time\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "result_file=open('queries_op.txt','w',encoding='utf-8')\n",
    "\n",
    "title_dict=defaultdict(str)\n",
    "text_score_dict=defaultdict(str)\n",
    "category_score_dict=defaultdict(str)\n",
    "title_score_dict=defaultdict(str)\n",
    "infobox_score_dict=defaultdict(str)\n",
    "references_score_dict=defaultdict(str)\n",
    "stopwords=defaultdict()\n",
    "\n",
    "\n",
    "title_file=open('title_names.txt','r',encoding='utf-8')\n",
    "\n",
    "body_scores_file=open('scored_body_index.txt','r',encoding='utf-8')\n",
    "title_scores_file=open('scored_title_index.txt','r',encoding='utf-8')\n",
    "infobox_scores_file=open('scored_infobox_index.txt','r',encoding='utf-8')\n",
    "reference_scores_file=open('scored_reference_index.txt','r',encoding='utf-8')\n",
    "category_scores_file=open('scored_category_index.txt','r',encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "stopwords_file=open('stopwords2.txt','r')\n",
    "stpwords=[line.rstrip() for line in stopwords_file]\n",
    "for word in stpwords:\n",
    "    stopwords[word]=True\n",
    "stopwords_file.close()\n",
    "\n",
    "\n",
    "\n",
    "for line in tqdm(title_file):\n",
    "    try:\n",
    "        ids,title=line.split('#')\n",
    "        title_dict[int(ids)]=title.rstrip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "for line in tqdm(body_scores_file):\n",
    "    word,scores=line.split('|')\n",
    "    text_score_dict[word]=scores.rstrip()\n",
    "    \n",
    "for line in tqdm(reference_scores_file):\n",
    "    word,scores=line.split('|')\n",
    "    references_score_dict[word]=scores.rstrip()\n",
    "    \n",
    "for line in tqdm(category_scores_file):\n",
    "    word,scores=line.split('|')\n",
    "    category_score_dict[word]=scores.rstrip()\n",
    "    \n",
    "for line in tqdm(title_scores_file):\n",
    "    word,scores=line.split('|')\n",
    "    title_score_dict[word]=scores.rstrip()\n",
    "    \n",
    "for line in tqdm(infobox_scores_file):\n",
    "    word,scores=line.split('|')\n",
    "    infobox_score_dict[word]=scores.rstrip()\n",
    "    \n",
    "title_file.close()\n",
    "body_scores_file.close()\n",
    "reference_scores_file.close()\n",
    "infobox_scores_file.close()\n",
    "category_scores_file.close()\n",
    "title_scores_file.close()\n",
    "\n",
    "\n",
    "def is_phrase_query(query):\n",
    "    ftq=['b:','c:','i:','r:','t:','l:']\n",
    "    for q in ftq:\n",
    "        if q in query:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def search_in_dictionary(word,dictionary_type):\n",
    "    if dictionary_type==\"body\":\n",
    "        return text_score_dict[word]\n",
    "    \n",
    "    if dictionary_type==\"category\":\n",
    "        return category_score_dict[word]\n",
    "    \n",
    "    if dictionary_type==\"infobox\":\n",
    "        return infobox_score_dict[word]\n",
    "    \n",
    "    if dictionary_type==\"title\":\n",
    "        return title_score_dict[word]\n",
    "\n",
    "    if dictionary_type==\"references\":\n",
    "        return references_score_dict[word]\n",
    "\n",
    "\n",
    "\n",
    "def get_one_word_scores(word,field_type):\n",
    "    word=nltk_stemmer.stem(word)\n",
    "    all_scores=search_in_dictionary(word,field_type)\n",
    "    \n",
    "    if all_scores != \"\":\n",
    "        scores=all_scores.split(',')\n",
    "        return scores\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def search_worst_case_query(query,num_results,sorted_ids,ordered_score_list):\n",
    "    worst_case_id_score=defaultdict(float)\n",
    "    print('got through worst case')\n",
    "    new_ids=[ids for id_lst in sorted_ids for ids in id_lst ]\n",
    "    \n",
    "    for ids in new_ids:\n",
    "        for dicts in ordered_score_list:\n",
    "            worst_case_id_score[ids]+=dicts[ids]\n",
    "    \n",
    "    new_id_scores=sorted(worst_case_id_score.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    \n",
    "    return new_id_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_titles(intersected_ids,query,sorted_ids,num_results,ordered_score_list):\n",
    "    copy=query\n",
    "    query=query.lower()\n",
    "    #print('printing via rel fun '+query)\n",
    "    words_in_query=set(query.split())\n",
    "    \n",
    "    relevant_ids=[]\n",
    "    \n",
    "    relevant_id_score=defaultdict(float)\n",
    "    \n",
    "    for id_list in sorted_ids:\n",
    "        for rel_id in id_list:\n",
    "            title_for_id=title_dict[rel_id].lower()\n",
    "            words_in_title=set(title_for_id.split())\n",
    "            #print(words_in_title)\n",
    "            if len(words_in_title.intersection(words_in_query))>=len(words_in_query):\n",
    "                relevant_ids.append(rel_id)\n",
    "    \n",
    "\n",
    "    for ids in relevant_ids:\n",
    "        for dicts in ordered_score_list:\n",
    "            relevant_id_score[ids]+=dicts[ids]\n",
    "    \n",
    "\n",
    "    relevant_id_score=sorted(relevant_id_score.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    return relevant_id_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_in_index(query,query_type,num_results):\n",
    "    query=query.lstrip()\n",
    "    query=query.rstrip()\n",
    "    query=query.lower()\n",
    "    words=list(set(query.split()))\n",
    "    \n",
    "    for word in stopwords.keys():\n",
    "        try:\n",
    "            words.remove(word)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    ordered_score_list=[]\n",
    "    sorted_score_list=[]\n",
    "\n",
    "    for word in words:\n",
    "   \n",
    "        ordered_score=defaultdict(float)\n",
    "\n",
    "        word_scores=get_one_word_scores(word,query_type)\n",
    "       \n",
    "        if word_scores!=\"\":\n",
    "            for item in word_scores:\n",
    "                ordered_score[int(item.split(':')[0])]=float(item.split(':')[1])\n",
    "\n",
    "\n",
    "            ordered_score_list.append(ordered_score)\n",
    "\n",
    "            ordered_set=sorted(ordered_score.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "            sorted_score_list.append(ordered_set)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "    sorted_ids=[]\n",
    "    for sorted_scores in sorted_score_list:\n",
    "        ids=[item[0] for item in sorted_scores]\n",
    "\n",
    "        sorted_ids.append(ids)\n",
    "\n",
    "    \n",
    "    #print(sorted_ids)\n",
    "    \n",
    "    intersected_ids=set(sorted_ids[0]).intersection(*sorted_ids)\n",
    "    \n",
    "    if len(intersected_ids) < num_results:\n",
    "        return get_relevant_titles(intersected_ids,query,sorted_ids,num_results,ordered_score_list)\n",
    "\n",
    "    \n",
    "    total_scores=defaultdict(float)\n",
    "    for ids in intersected_ids:\n",
    "        for dicts in ordered_score_list:\n",
    "            total_scores[ids]+=dicts[ids]\n",
    "    \n",
    "    #print('printing  normally '+query)\n",
    "\n",
    "    final_scores=sorted(total_scores.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "    return final_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_phrase_query(query,num_results):\n",
    "    \n",
    "    titles=search_in_index(query,'title',num_results)\n",
    "    bodys=search_in_index(query,'body',num_results)\n",
    "    refs=search_in_index(query,'references',num_results)\n",
    "    cats=search_in_index(query,'category',num_results)\n",
    "    infos=search_in_index(query,'infobox',num_results)\n",
    "    \n",
    " \n",
    "    \n",
    "    phrase_dict=defaultdict(float)\n",
    "   \n",
    "    \n",
    "    for item in titles:\n",
    "        phrase_dict[item[0]]+=(item[1])\n",
    "        \n",
    "    for item in bodys:\n",
    "        phrase_dict[item[0]]+=(item[1])\n",
    "    \n",
    "    for item in refs:\n",
    "        phrase_dict[item[0]]+=(item[1])\n",
    "    \n",
    "    for item in cats:\n",
    "        phrase_dict[item[0]]+=(item[1])\n",
    "    \n",
    "    for item in infos:\n",
    "        phrase_dict[item[0]]+=(item[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    top_ids=sorted(phrase_dict.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    #print(top_ids)\n",
    "    top_results=[(ids[0],title_dict[ids[0]]) for ids in top_ids]\n",
    "\n",
    "    return top_results\n",
    "\n",
    "def search_field_query(query,num_results):\n",
    "    query_list=re.split('(b:|i:|c:|r:|t:|l:)',query)\n",
    "    #print(query_list)\n",
    "    reference_q=None\n",
    "    text_q=None\n",
    "    category_q=None\n",
    "    infobox_q=None\n",
    "    title_q=None\n",
    "    link_q=None\n",
    "\n",
    "    reference_query=None\n",
    "    text_query=None\n",
    "    category_query=None\n",
    "    infobox_query=None\n",
    "    title_query=None\n",
    "    link_query=None\n",
    "    try:\n",
    "        reference_q=query_list.index('r:')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        text_q=query_list.index('b:')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        category_q=query_list.index('c:')\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        infobox_q=query_list.index('i:')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        title_q=query_list.index('t:')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        link_q=query_list.index('l:')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    rf=0\n",
    "    bf=0\n",
    "    tf=0\n",
    "    cf=0\n",
    "    if_=0\n",
    "    lf=0\n",
    "    \n",
    "\n",
    "    if reference_q:\n",
    "        rf=1\n",
    "        reference_query=query_list[reference_q+1]\n",
    "    \n",
    "    if link_q:\n",
    "        lf=1\n",
    "        link_query=query_list[link_q+1]\n",
    "    \n",
    "    if text_q:\n",
    "        bf=1\n",
    "        text_query=query_list[text_q+1]\n",
    "\n",
    "    if category_q:\n",
    "        cf=1\n",
    "        category_query=query_list[category_q+1]\n",
    "\n",
    "    if infobox_q:\n",
    "        if_=True\n",
    "        infobox_query=query_list[infobox_q+1]\n",
    "\n",
    "    if title_q:\n",
    "        tf=1\n",
    "        title_query=query_list[title_q+1]\n",
    "\n",
    "    \n",
    "    ref_res=[]\n",
    "    info_res=[]\n",
    "    title_res=[]\n",
    "    cat_res=[]\n",
    "    link_res=[]\n",
    "    text_res=[]\n",
    "    \n",
    "    ref_title=[]\n",
    "    info_title=[]\n",
    "    title_title=[]\n",
    "    cat_title=[]\n",
    "    link_title=[]\n",
    "\n",
    "    body_title=[]\n",
    "    \n",
    "    intersected_ids=[]\n",
    "\n",
    "    field_query_start_time=time()\n",
    "    \n",
    "    if(reference_q):\n",
    "        #print('r ',reference_query.rstrip())\n",
    "        ref_res=search_in_index(reference_query,'references',num_results)\n",
    "        ref_title=[(ids[0],title_dict[ids[0]]) for ids in ref_res]\n",
    "        intersected_ids.append([ids[0] for ids in ref_res])\n",
    "\n",
    "    if(link_q):\n",
    "        #print('r ',reference_query.rstrip())\n",
    "        link_res=search_in_index(link_query,'references',num_results)\n",
    "        link_title=[(ids[0],title_dict[ids[0]]) for ids in link_res]\n",
    "        intersected_ids.append([ids[0] for ids in link_res])\n",
    "    \n",
    "    if(infobox_q):\n",
    "        #print('i ',infobox_query.rstrip())\n",
    "        info_res=search_in_index(infobox_query,'infobox',num_results)\n",
    "        info_title=[(ids[0],title_dict[ids[0]]) for ids in info_res]\n",
    "        intersected_ids.append([ids[0] for ids in info_res])    \n",
    "        \n",
    "    if(text_q):\n",
    "        #print('t ',text_query.rstrip())\n",
    "        text_res=search_in_index(text_query,'body',num_results)\n",
    "        body_title=[(ids[0],title_dict[ids[0]]) for ids in text_res]\n",
    "        intersected_ids.append([ids[0] for ids in text_res])\n",
    "\n",
    "    if(title_q):\n",
    "        #print('T ',title_query.rstrip())\n",
    "        title_res=search_in_index(title_query,'title',num_results)\n",
    "        \n",
    "        title_title=[(ids[0],title_dict[ids[0]]) for ids in title_res]\n",
    "        intersected_ids.append([ids[0] for ids in title_res])\n",
    "    \n",
    "    if(category_q):\n",
    "        #print('c ',category_query.rstrip())\n",
    "        cat_res=search_in_index(category_query,'category',num_results)\n",
    "        cat_title=[(ids[0],title_dict[ids[0]]) for ids in cat_res]\n",
    "        intersected_ids.append([ids[0] for ids in cat_res])\n",
    "        \n",
    "    \n",
    "    \n",
    "    final_results=set(intersected_ids[0]).intersection(*intersected_ids)\n",
    "\n",
    "    extras=[]\n",
    "\n",
    "    results=[(item,title_dict[item]) for item in final_results]\n",
    "\n",
    "    if len(results) >= num_results:\n",
    "          field_query_end_time=time()\n",
    "\n",
    "          total_time=field_query_end_time - field_query_start_time\n",
    "          average_time_per_query=(total_time)/(if_+lf+tf+cf+bf+rf)\n",
    "          \n",
    "          return results,total_time,average_time_per_query\n",
    "\n",
    "    else:\n",
    "        if tf > 0:\n",
    "            extras.append(search_phrase_query(title_query,num_results)[:num_results])\n",
    "\n",
    "        if if_ > 0:\n",
    "            extras.append(search_phrase_query(infobox_query,num_results)[:num_results])\n",
    "        \n",
    "        if cf > 0:\n",
    "            extras.append(search_phrase_query(category_query,num_results)[:num_results])\n",
    "        \n",
    "        if lf > 0:\n",
    "            extras.append(search_phrase_query(link_query,num_results)[:num_results])\n",
    "        \n",
    "        if rf > 0:\n",
    "            extras.append(search_phrase_query(reference_query,num_results)[:num_results])\n",
    "        \n",
    "        if bf > 0:\n",
    "            extras.append(search_phrase_query(text_query,num_results)[:num_results])\n",
    "                \n",
    "\n",
    "        union_results=list(set(extras[0]).union(*extras))\n",
    "        \n",
    "       \n",
    "\n",
    "        results=results+union_results\n",
    "\n",
    "        field_query_end_time=time()\n",
    "\n",
    "        total_time=field_query_end_time - field_query_start_time\n",
    "        average_time_per_query=(total_time)/(if_+lf+tf+cf+bf+rf)\n",
    "\n",
    "    return results,total_time,average_time_per_query\n",
    "\n",
    "\n",
    "def execute_query(query,num_results):\n",
    "    if is_phrase_query(query):\n",
    "        start=time()\n",
    "        phrase_result=search_phrase_query(query,num_results)\n",
    "        end=time()\n",
    "\n",
    "        for i in range(num_results):\n",
    "            result_file.write(f'{phrase_result[i][0]},{phrase_result[i][1]}\\n')\n",
    "\n",
    "        result_file.write(f'{end-start},{end-start}\\n')\n",
    "        result_file.write('\\n')\n",
    "\n",
    "\n",
    "        #print(f'Phrase Query Results \\n {phrase_result[:num_results]}')\n",
    "    \n",
    "    else:\n",
    "        #title_title,body_title,cat_title,info_title,ref_title,link_title=search_field_query(query,num_results)\n",
    "        \n",
    "        top_field_scores,total_time,average_time_per_query=search_field_query(query,num_results)\n",
    "        \n",
    "\n",
    "\n",
    "        for i in range(num_results):\n",
    "            result_file.write(f'{top_field_scores[i][0]},{top_field_scores[i][1]}\\n')\n",
    "\n",
    "        result_file.write(f'{total_time},{average_time_per_query}\\n')\n",
    "        result_file.write('\\n')\n",
    "        # print(top_field_scores[:num_results])\n",
    "        # print(total_time,average_time_per_query)\n",
    "\n",
    "\n",
    "\n",
    "query_file=sys.argv[1]\n",
    "\n",
    "\n",
    "query_file=open(query_file,'r',encoding='utf-8')\n",
    "for queries in query_file:\n",
    "    num_results,query=queries.split(',')\n",
    "    print(num_results,query.rstrip())\n",
    "    execute_query(query.rstrip(),int(num_results))\n",
    "    print('')\n",
    "\n",
    "query_file.close()\n",
    "\n",
    "\n",
    "result_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
